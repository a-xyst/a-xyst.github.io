---
title:  "MLiA读后笔记"
excerpt: "不太满意，书里代码注释太少，有很多地方不明白为什么要那么写。不过跟着把里面算法都实现了一遍，对抽象知识的实现有了一点概念，还是有所收获的。做点笔记，不谈原理，只记录应用各算法的注意事项。"
toc: true
toc_label: "目录"
toc_icon: "gear"
categories:
  - Machine_Learning
tags:
  
---


## 知识点分类

### 监督

朴素贝叶斯适合处理文本分类问题。

SVM、AdaBoost在监督模型中较为强大。

SVM、AdaBoost处理多标签分类需对原始模型进行修改。

PCA、SVD适用于连续型数据。

#### 分类

K-近邻、决策树、朴素贝叶斯(离散)、Logistic回归、SVM、AdaBoost

#### 回归

线性回归、树回归

### 无监督

K-均值(连续,聚类)、Apriori(关联分析)、FP-growth(离散,关联分析)

### 降维技术

PCA、SVD


## 知识点介绍

### K-近邻

优点：对异常值不敏感。

缺点：时空复杂度高。

适用数据类型：离散型、连续型

伪代码：
```
对未知类别属性的数据集中每个点依次执行：
    计算已知类别数据集中的点与当前点间距离
    将距离排序
    选择与当前点距离最小的K个点
    计算前K个点所在类别的出现频率
    返回前K个点中出现频率最高的类别作为当前点的预测值
```

### 决策树

优点：时间复杂度低，输出易理解，对中间值缺失不敏感，可处理不相关特征。

缺点：易产生过拟合。

适用数据类型：离散型、连续型（需离散化）

产生决策树的分类算法有：ID3, C4.5等。

伪代码：
```
创建根结点
执行以下算法直到达到停止条件：
    根据所选择的评估函数（例：信息增益），选择分类效果最好的属性作为当前划分的属性
    按该属性把当前结点分为若干子集，每个取值形成一棵字数，递归划分
    
停止条件包括：
结点中所有样本分类相同；
结点剩余属性数==1，以样本中出现次数最多的分类作为结点的分类；
```

### 朴素贝叶斯

优点：适用于小数据集，可处理多标签分类。

缺点：对数据形式敏感。

适用数据类型：离散型

公式：

$$P(c_i|x,y)=\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$

### Logistic回归

优点：计算代价低。

缺点：易于欠拟合。

适用数据类型：离散型、连续型

公式：

$$\sigma(z)=\frac{1}{1+e^{-z}}$$

#### 梯度法

公式：

$$w=w+a\nabla_wf(w)$$

伪代码：
```
初始化回归系数
迭代给定次数：
    计算数据集梯度
    使用步长、梯度值更新回归系数
返回回归系数
```

#### 随机梯度法 

伪代码：
```
初始化回归系数
对数据集中每个样本：
    计算样本梯度
    使用步长、梯度值更新回归系数
返回回归系数
```

### SVM

优点：不易过拟合、计算开销低、结果易解释。

缺点：对参数和核函数的选择敏感。

适用数据类型：离散型、连续型

比较复杂，参考[SVM](https://en.wikipedia.org/wiki/Support_vector_machine)、[SMO](https://en.wikipedia.org/wiki/Sequential_minimal_optimization)

### AdaBoost

优点：不易过拟合。

缺点：对异常值敏感。

适用数据类型：离散型、连续型

伪代码：

```
迭代直到错误率小于阈值：
    根据选择的基础模型（弱分类器），找到当前模型的最佳参数
    将当前模型加入模型集
    计算当前模型的权重和数据集的权重
    更新累计类别估计值
```

### 线性回归

优点：结果易于理解、计算代价低。

缺点：不适用于非线性数据。

适用数据类型：离散型、连续型

公式：

$$\hat w=(X^TX)^{-1}X^Ty$$ 

（[最小二乘法](https://en.wikipedia.org/wiki/Least_squares)）

#### 局部加权线性回归

防止欠拟合。

公式：$$\hat w=(X^TWX)^{-1}X^TWy$$

W为使用高斯核的权重矩阵，

$$W(i,i)=exp(\frac{|x^{(i)}-x|}{-2k^2})$$

#### 岭回归

减少特征数。

公式：$$\hat w=(X^TX+\lambda I)^{-1}X^Ty$$，$$\lambda$$为惩罚项

#### 前向逐步回归

减少特征数。

伪代码：
```
数据标准化
迭代直到误差小于阈值：
    当前最小误差=inf
    对每个特征：
        正负：
            根据步长、符号改变当前特征的系数
            计算新误差，更新最小误差
        更新系数
```

### 分类回归树(CART)

优点：适用于非线性数据。

缺点：结果不易理解。

适用数据类型：离散型、连续型

伪代码：
```
创建根结点
对于当前结点：
    如果不能切分，存为叶节点
    根据选择的评估函数进行二元切分
    递归切分左子树
    递归切分右子树
```

#### 后剪枝

相对于前剪枝，优点在于对参数选择不敏感。

伪代码：
```
基于已有的切分测试数据：
    如果存在子树，递归剪枝
    计算合并当前两个子结点的误差
    计算不合并的误差
    如果合并后误差小，则合并
```

#### Logistic模型树(LMT)

四个翻译还能翻丢一个词，害我查了好久。

用线性模型对叶节点建模。

### K-均值

优点：

缺点：收敛速度较慢，且可能收敛到局部最小值。

适用数据类型：连续型

伪代码：
```
随机创建k个质心，作为起始质心
当任意一个数据点的簇分配结果发生改变：
    对每个数据点：
        对每个质心：
            计算质心与数据点之间的距离
        将数据点分配到距其最近的簇
    对每个簇，计算簇中所有点位置的均值并将它作为质心
```

#### 二分K-均值

防止收敛于局部最小值
伪代码：
```
创建初始簇
当簇数目小于k：
    对每个簇：
        计算总误差
        在当前簇上进行K-均值划分(k=2)
        计算划分后总误差
    选择总误差最小的hua'fen
```

### Apriori

优点：

缺点：时间复杂度较高。

适用数据类型：离散型、连续型

伪代码：
```
当集合非空：
    构建一个每个元素由k个项组成的候选项集列表
    去除非频繁候选项集
    构建k+1项候选项集列表
```

### FP-Growth

优点：时间复杂度较低。

缺点：泛用性较低。

适用数据类型：离散型

伪代码：
```
FP树的创建：
遍历数据集，去除不满足最小支持度的元素
遍历频繁数据集，把每个项目添加到已存在的路径，路径不存在时创建新路径
（类似于建一棵字典树？区别在于存在横向指针，相似项互相连接，第一项存放在头指针表中）
抽取频繁项集：
迭代直到树包含一个元素项：
    从FP树中获取前缀集
    利用前缀集创建一个FP树
```

### PCA

标准化数据，计算协方差矩阵的特征值和特征向量，选择前N个最大的特征值，使用它们对应的特征向量构建新空间下的数据。

### SVD

把原矩阵分解为左中右三个，保留左矩阵的前N列/中矩阵的前N行和前N列/右矩阵的前N行即可降维。
左矩阵把行映射到新空间，右矩阵把列映射到新空间，中矩阵是原矩阵的奇异值阵。