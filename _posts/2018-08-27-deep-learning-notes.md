---
title:  "深度学习笔记"
excerpt: "notes of Deep Learning Book"
toc: true
toc_label: "目录"
toc_icon: "gear"
categories:
  - Deep_Learning
tags:
  
---

## 第2章 线性代数

范数衡量向量的大小；Frobenius范数衡量矩阵的大小

**标准正交**：向量都互相正交，而且范数都为1. 正交矩阵指行向量和列向量分别标准正交的方阵，即A^T=A^-1

每个实对称矩阵都可以分解成实特征向量和实特征值A=Q \Lambda Q^T

半正定（特征值非负）矩阵保证x^T A x=>=0

存在零特征值<=>矩阵奇异

**SVD**：U（列向量：左奇异向量）D（对角：奇异值）V^T（列向量：右奇异向量）可推出M-P求逆

**Moore-Penrose伪逆**：求非方矩阵的伪逆 A^+=VD^+U^T（D^+：D的非零元素取倒数后转置）

**PCA**：降维，提取特征

1. 标准化(减均值除标准差)
2. 求协方差矩阵（因已标准化，其协方差矩阵为相关矩阵）
3. 求协方差矩阵的特征值及相应的特征向量
4. 选择主成分（用方差贡献率c_i= \lambda_i/\sum \lambda来解释主成分所反映的信息量大小，选择前m个主成分，以累计贡献率\sum c达到足够大的阈值为原则（一般为85%/80%）

## 第3章 概率与信息论

**协方差**：绝对值大代表变量值变化大，并且同时距离各自的均值很远；正代表倾向于同时取得较大值，反之亦然

协方差表现了变量间的线性关系；独立性表现了线性和非线性关系

需要对概率密度函数求值时可以用精度（方差的倒数）替代方差，同理对多维正态分布的概率密度函数求值时可以用精度矩阵替代协方差矩阵

**指数分布**：可以在x=0处取得边界点

**Laplace分布**：允许在任意一点\mu处设置概率质量的峰值

**Dirac分布**：概率分布中所有质量都集中在一个点上(x=\mu)，经常作为经验分布的一个组成部分（经验分布是训练数据的最大似然概率密度函数）

任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近

sigmoid函数\sigma，softplus函数\zeta的一些性质：

**零测度**：在高维空间中，低维图形的测度为0. 某个性质如果几乎处处都成立，那么它在测度为0的集合以外都成立

自信息表达了观测到一个事件时获得的信息量；一个分布的香农熵指遵循这个分布的事件所产生的期望信息总量

**KL散度**：衡量两个分布之间的差异，为非负值，不对称

假设希望用q来近似p，如果最小化D_KL(p\|\|q)，就是选择一个q使得它在p具有高概率的地方具有高概率，反之如果最小化D_KL(q\|\|p)，就是选择一个q使得它在p具有低概率的地方具有低概率

交叉熵是忽略左边一项的KL散度，特定情况下可以代替交叉熵被用于计算代价来减小计算量，例：如果针对Q优化，最小化交叉熵H(P,Q)等价于最小化KL散度

0log0=0（lim_{x to 0} x logx=0） 

**结构化概率模型**：有向图表示=信念网络/贝叶斯网络，无向图表示=马尔可夫随机场

## 第4章 数值计算

**上溢和下溢**：前者是大数被近似为无限，后者是小数被近似为0。计算softmax函数时，必须对上溢和下溢进行数值稳定（计算softmax(z), z=x-max x_i, 单独实现log softmax(x) 防止计算log0）

**病态条件**：输入的轻微扰动就可造成输出的巨大变化

**基于梯度的优化方法**：

曲率（二阶导数值）影响代价函数的下降速度，曲率为负（函数凸起）则下降得快，反之亦然

**偏导数矩阵**：

Jacobian矩阵=一阶偏导数矩阵，Hessian矩阵=二阶偏导数矩阵

因为微分算子在任何二阶偏导数连续的点处可以交换，所以应用中大多数Hessian矩阵都是实对称的，所以可以将其分解成一组实特征值和特征向量的正交基

在特定方向d上的二阶导数可以写成d^THd, 当d是H的一个特征向量，二阶导数就是对应的特征值，否则方向二阶导数是所有特征的加权平均，权重在0和1之间，且与d夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征确定最小二阶导数。

在当前点x_0处作二阶泰勒展开，

$$
f(x) \approx f(x_0)+(x-x_0)^T g+\frac{1}{2}(x-x_0)^TH(x-x_0)
$$
其中g是梯度，H是x_0点的Hessian。把x_0近似代入x，可得
$$
f(x_0-\epsilon g) \approx f(x_0)-\epsilon g^T g+\frac{1}{2}\epsilon^2g^THg
$$
这三项分别为：函数原始值、斜率导致的预期下降值，曲率导致的上升值。最后一项太大时，梯度下降可能会朝f增加的方向移动；为0时代表不会出现这类现象，增大学习率一定使f下降。

当g^THg为正，计算可得最优步长为g^Tg/g^THg

g与H最大特征值\lambda_max对应的特征向量对齐，则最优步长是1/\lambda_max。当f能用二次函数很好地近似时，Hessian的特征值决定了学习率的量级。

二阶导数测试：确定临界点的类型。二阶导数>0, 局部极小点；<0，局部极大点；=0，可能是鞍点。

多维情况下的二阶导数测试：检测Hessian矩阵的特征值。当矩阵正定，局部极小点（方向二阶导数在所有方向都是正的）；负定，局部极大点

Hessian的条件数（最大、最小特征值之比）衡量单个点上每个方向的二阶导数的变化范围，条件数很小时梯度下降法表现较差，因为不同方向上的倒数增加速度相差较大，病态条件导致难以选择合适的步长（太大的步长可能直接冲过极小点）

可使用Hessian的信息来指导搜索，如牛顿法（通过泰勒展开来迭代地近似原函数：f(x_n)+(x_(n+1)-x_n)f'(x_n)=0, 则x_(n+1)=x_n-(f(x_n)/f'(x_n))。



